

**Entry to NLP**

ğŸŸ¢ **Character Encoding Issues:**  
ğŸ”¹ ASCII assigns numerical values to letters but **doesnâ€™t capture meaning**.  
ğŸ”¹ Example: *listen* & *silent* share the same letters but **have different meanings** â†’ Letter-based encoding fails.  

ğŸŸ¡ **Word-Based Encoding:**  
ğŸ”¹ Assign **unique values** to words â†’ Helps the model **recognize sentence similarities**.  
ğŸ”¹ Example:  
   - `"I love my dog"` â†’ **1,2,3,4**  
   - `"I love my cat"` â†’ **1,2,3,5**  
   - ğŸ§ The **first three words match**, showing similarity between sentences.  

ğŸ”µ **Deep Learning Integration:**  
ğŸ”¹ This method **helps train neural networks effectively**.  
ğŸ”¹ âœ… **TensorFlow & Keras provide APIs** to make word encoding easy.  


### **ğŸ“Œ What is a Token in NLP?**  
A **token** is a **smallest unit** of text in NLP that carries meaning. It can be:  
ğŸ”¹ A **word** (e.g., "machine", "learning")  
ğŸ”¹ A **subword** (e.g., "un-", "happiness")  
ğŸ”¹ A **character** (e.g., "H", "e", "l", "l", "o")  

For example, in the sentence:  
ğŸ“Œ `"I love NLP!"`  
ğŸ‘‰ Tokens: `["I", "love", "NLP", "!"]`  

---

### **ğŸ“Œ What is Tokenization?**  
Tokenization is the process of **splitting text into tokens** to make it usable for NLP models.

### **ğŸ”¹ Types of Tokenization**
1ï¸âƒ£ **Word Tokenization**  
   - Splits text into words.  
   - Example:  
     ```python
     from nltk.tokenize import word_tokenize
     text = "I love NLP!"
     tokens = word_tokenize(text)
     print(tokens)  # ['I', 'love', 'NLP', '!']
     ```
  
2ï¸âƒ£ **Subword Tokenization**  
   - Breaks words into meaningful subwords.  
   - Example: `"unhappiness"` â†’ `["un", "happiness"]`  
   - Used in **BPE (Byte Pair Encoding), WordPiece (BERT), and SentencePiece (T5, GPT-3).**  

3ï¸âƒ£ **Character Tokenization**  
   - Breaks text into **individual characters**.  
   - Example: `"Chat"` â†’ `['C', 'h', 'a', 't']`  
   - Useful for languages with **no spaces** (e.g., Chinese, Japanese).  

---

### **ğŸ“Œ Why is Tokenization Important?**
âœ… Prepares text for NLP models.  
âœ… Converts **unstructured** text into a **structured format**.  
âœ… Helps in **vectorization** (e.g., word embeddings).  




















### **ğŸ”¹ What Does `adapt()` Do in Simple Terms?**  

Think of `adapt()` as a way for **a preprocessing layer to learn from your data before training**. Instead of giving the model a fixed set of rules, `adapt()` **analyzes your dataset and adjusts the layerâ€™s behavior accordingly**.  

### **ğŸ”¹ Why Do We Need `adapt()`?**  
When using layers like `TextVectorization`, we donâ€™t manually tell the layer **which words exist in the dataset**. Instead, `adapt()` lets the layer **learn the vocabulary** from the dataset before training.  

---

### **ğŸ”¹ Example: How `adapt()` Works in NLP**
#### **1ï¸âƒ£ Without `adapt()` (Model Doesnâ€™t Know Words)**
```python
import tensorflow as tf

vectorizer = tf.keras.layers.TextVectorization()

text = tf.constant(["Hello TensorFlow"])  
print(vectorizer(text))  # âŒ Won't work because the layer doesnâ€™t know the vocabulary!
```
ğŸ‘ The model has **no idea what â€œHelloâ€ or â€œTensorFlowâ€ means**.

---

#### **2ï¸âƒ£ With `adapt()` (Model Learns Words First)**
```python
import tensorflow as tf

# Create TextVectorization layer
vectorizer = tf.keras.layers.TextVectorization()

# Prepare a dataset of text examples
dataset = tf.data.Dataset.from_tensor_slices([
    "Hello world!",
    "Deep learning with NLP",
    "TensorFlow is amazing"
])

# Learn vocabulary from dataset
vectorizer.adapt(dataset)

# Now the layer knows the words!
print(vectorizer(["Hello TensorFlow"]))  
```
âœ… **Now it works!** The words `"Hello"` and `"TensorFlow"` are converted into numbers **based on the learned vocabulary**.

---

### **ğŸ”¹ What `adapt()` Does in Simple Words**
âœ… It **looks at your dataset** before training.  
âœ… It **finds patterns** (e.g., what words appear most).  
âœ… It **stores this knowledge** inside the layer.  
âœ… It **ensures consistent preprocessing** when training your model.

Think of `adapt()` like a chef **tasting ingredients before cooking**â€”it helps the model **understand the dataset before learning!**  


